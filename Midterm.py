import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from mpl_toolkits.mplot3d import Axes3D
np.random.seed(1)                               # set random seed for debuging
tf.set_random_seed(1)
############################################################################################################
data = np.zeros([500, 3])
mean = np.array([[0, 0, 0], [0, 0, 5], [0, 5, 0], [1, 5, 5]])
for i in range(4):
    cur = np.random.multivariate_normal(mean[i, :], np.identity(3), (125, 1)).reshape(125,3)  # generate random data
    data[i * 125:(i+1)* 125, :] = cur           # shape 500, 3
############################################################################################################
fig = plt.figure()
ax1 = fig.add_subplot(231, projection='3d')
size = [75]*500
color = [1]*125 + [2]*125 + [3]*125 + [4]*125
ax1.scatter(data[:, 0], data[:, 1], data[:, 2], s=size, c=color)   # 3D scatter plot
ax1.set_xlabel('X')
ax1.set_ylabel('Y')
ax1.set_zlabel('Z')
ax1.set_title('Q2')
############################################################################################################
covData = np.dot(data.T, data) / 500
u, s, v = np.linalg.svd(covData)              # u is eigen vector
print('u', u[:, 0:2].T)
reducedData = np.dot(data, u[:, 0:2])        # generate reduced data

ax2 = fig.add_subplot(232)
ax2.scatter(reducedData[:, 0], reducedData[:, 1], s=size, c=color)
ax2.set_xlabel('X')
ax2.set_ylabel('Y')
ax2.set_title('Q4- Data from PCA')
# plt.show()
###########################################################################################################
# set up NN in tensorflow
# a single layer NN is enough, because we only need to determine a 3 by 2 Matrix
def NN(train, label, iteration):
    X_train, Y_train = np.float32(train), np.float32(label)
    def placeHolder():
        X = tf.placeholder(tf.float32, [None, 3], name='X')
        Y = tf.placeholder(tf.float32, [None, 2], name='Y')
        return X, Y
    def initialize():   # initialize W and b
        W = tf.get_variable("W", [2, 3], initializer=tf.contrib.layers.xavier_initializer(seed=1))
        return W
    def forwardPropagation(x, w):
        z = tf.matmul(w, tf.transpose(x))
        return z
    def calCost(z, y):
        cost = np.sum(np.abs(tf.transpose(z)-y))
        print(cost)
        return cost

    X, Y = placeHolder()
    W = initialize()
    z = forwardPropagation(X, W)
    cost = calCost(z, Y)
    optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)
    init = tf.global_variables_initializer()
    with tf.Session() as sess:
        sess.run(init)
        for turn in range(iteration):
            for i in range(50):
                xt = X_train[i*10:i*10+10, :]
                yt = Y_train[i*10:i*10+10, :]
                sess.run(optimizer, feed_dict={X:xt, Y:yt})
            tW = sess.run(W)
            accuracy = np.dot(X_train, tW.T) - Y_train
            print('Current Iteration ', turn, 'Current Cost ', np.average(np.abs(accuracy)))
        tW = sess.run(W)
        print(tW)
        return tW
# W = NN(data, reducedData, 15)
'''
After training, W is 
[[-0.1037042  -0.72249693 -0.6823656 ]
 [ 0.01259715 -0.69454783  0.72085124]]
 mean absolute error is 0.014
 the origional U is 
 [[-0.11263086 -0.71572222 -0.68924306]
 [ 0.00885393 -0.69435223  0.71958084]]
'''
print(data.shape)
W = np.array([[-0.1037042,  -0.72249693, -0.6823656 ],
 [ 0.01259715 ,-0.69454783 , 0.72085124]])
trainedData = np.dot(W , data.T).T       # 2*3  *   500*3.T   = 2*500
ax3 = fig.add_subplot(233)
print(trainedData.shape)
ax3.scatter(trainedData[:, 0], trainedData[:, 1], s=size, c=color)
ax3.set_xlabel('X')
ax3.set_ylabel('Y')
ax3.set_title('Q8-Data from NN PCA')

# Q9, from the plot, the difference is hard to be noticed by human eyes
#    unlike standard PCA, NN PCA is based on learning from training sample, which is generated by standard PCA.
#    Because we already know that the standard PCA is in the shape of (3,2), so we only generated an one-layer NN,
#    and generated good result
######################################################################################################################
label = [0]*125 + [1] * 125 + [2] * 125 + [3] * 125
lda = LinearDiscriminantAnalysis(solver="svd", store_covariance=True, n_components=2)

newData = np.array(lda.fit(data, label).transform(data))
print('coef', lda.coef_)
ax4 = fig.add_subplot(234)
ax4.scatter(newData[:, 0], newData[:, 1], s=size, c=color)
ax4.set_xlabel('X')
ax4.set_ylabel('Y')
ax4.set_title('Q10-Data from LDA')
print('#', newData.shape)
# newW = NN(data, newData, 30)
newW = [[ 0.89537019,  0.19827998 , 0.1902625 ],
 [-0.07452867, -0.67853248 , 0.70091814]]
trainedData2 = np.dot(newW , data.T).T
ax5 = fig.add_subplot(235)
ax5.scatter(trainedData2[:, 0], trainedData2[:, 1], s=size, c=color)
ax5.set_xlabel('X')
ax5.set_ylabel('Y')
ax5.set_title('Q10-Data from NN LDA')


plt.show()
'''
compare to PCA, the performance of NN LDA is not as good as NN PCA, mean error = 0.87 vs 0.014  
after training W is 
[[ 0.89537019  0.19827998  0.1902625 ]
 [-0.07452867 -0.67853248  0.70091814]]
'''
